{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b143cc",
   "metadata": {},
   "source": [
    "#### This is the simplest training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "301437dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: Tesla V100-SXM2-16GB, gpu_id: 3\n",
      "number of parameters: 0.02M\n",
      "Entered Original mode\n",
      "Entered Original mode\n",
      "Entered Original mode\n",
      "Entered Original mode\n",
      "Entered Original mode\n",
      "Entered Original mode\n",
      "Epoch 10 completed in 0.28 seconds\n",
      "Epoch 20 completed in 0.27 seconds\n",
      "Epoch 30 completed in 0.27 seconds\n",
      "Epoch 40 completed in 0.25 seconds\n",
      "Epoch 50 completed in 0.23 seconds\n",
      "Epoch 60 completed in 0.29 seconds\n",
      "Epoch 70 completed in 0.25 seconds\n",
      "Epoch 80 completed in 0.25 seconds\n",
      "Epoch 90 completed in 0.23 seconds\n",
      "Epoch 100 completed in 0.26 seconds\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model_hyperbolic import GPT, GPTConfig  \n",
    "import datetime\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = '3' # select a single GPU\n",
    "    # gpu_id = '2,3' # select multiple GPUs\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))\n",
    "\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    gpu_id = -1 # select CPU\n",
    "\n",
    "# Argument parsing\n",
    "# parser = argparse.ArgumentParser(description=\"Train nanoGPT on Tao Te Ching\")\n",
    "# parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for training (default: 32)\")\n",
    "# parser.add_argument(\"--epochs\", type=int, default=10, help=\"Number of epochs to train (default: 10)\")\n",
    "# parser.add_argument(\"--learning_rate\", type=float, default=0.003, help=\"Learning rate (default: 0.003)\")\n",
    "# parser.add_argument(\"--block_size\", type=int, default=32, help=\"Context size (default: 32)\")\n",
    "# parser.add_argument(\"--n_embd\", type=int, default=16, help=\"Embedding dimension (default: 16)\")\n",
    "# parser.add_argument(\"--mode\", type=str, default='original', help=\"Attention mode (default: original)\")\n",
    "args = dict(n_layer=6, \n",
    "            epochs = 100,\n",
    "              n_head=8, \n",
    "              n_embd=16, \n",
    "              block_size=32, \n",
    "              batch_size=32, \n",
    "              bias=False, \n",
    "              dropout=0.0, \n",
    "              learning_rate=0.0003, \n",
    "              mode='original')\n",
    "\n",
    "# Dataset preparation\n",
    "class TaoTeChingDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
    "        self.block_size = block_size\n",
    "        self.data = [self.stoi[ch] for ch in data]\n",
    "        self.vocab_size = len(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        dix = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        target = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return dix, target\n",
    "\n",
    "# FULL TAO DATASET\n",
    "\n",
    "# with open('data/tao.txt', 'r', encoding='utf-8') as f:\n",
    "#     full_data = f.read()\n",
    "# chars = sorted(list(set(full_data)))\n",
    "# full_dataset = TaoTeChingDataset(full_data, chars, block_size=args['block_size'])\n",
    "\n",
    "# split_idx = int(len(full_data) * 0.9)\n",
    "\n",
    "# train_dataset = TaoTeChingDataset(full_data[:split_idx], chars, block_size=args['block_size'])\n",
    "# val_dataset = TaoTeChingDataset(full_data[split_idx:], chars, block_size=args['block_size'])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, shuffle=True, batch_size=args['batch_size'])\n",
    "# val_loader = DataLoader(val_dataset, shuffle=False, batch_size=args['batch_size'])  \n",
    "\n",
    "\n",
    "# SMALL TAO DATASET\n",
    "\n",
    "with open('data/tao.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    chars = sorted(list(set(data)))\n",
    "    overfit_data = data[17:301]\n",
    "    overfit_val_data = data[306:474]\n",
    "    \n",
    "overfit_dataset = TaoTeChingDataset(overfit_data, block_size=args.block_size)\n",
    "overfit_loader = DataLoader(overfit_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "overfit_val_dataset = TaoTeChingDataset(overfit_val_data, block_size=args.block_size)\n",
    "overfit_val_loader = DataLoader(overfit_val_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "model_args = dict(n_layer=6, \n",
    "                  n_head=8, \n",
    "                  n_embd=16, \n",
    "                  block_size=args['block_size'], \n",
    "                  bias=False, \n",
    "                  vocab_size=full_dataset.vocab_size, \n",
    "                  dropout=0.0, \n",
    "                  mode=args['mode'])\n",
    "\n",
    "model = GPT(GPTConfig(**model_args)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(model, epoch, loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, loss = model(data, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "#         if idx % 100 == 0:\n",
    "#             print(f\"Epoch: {epoch} | Loss: {loss.item()}\")\n",
    "    train_loss = train_loss/len(loader)\n",
    "    return train_loss\n",
    "        \n",
    "            \n",
    "def evaluate(model, epoch, loader):\n",
    "    model.eval()  \n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  \n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            _, loss = model(data, target)\n",
    "            val_loss += loss.item()  \n",
    "            \n",
    "    val_loss /= len(loader) \n",
    "    return val_loss\n",
    "\n",
    "def save_checkpoint(model, optimizer, model_args, train_losses, val_losses, out_dir, args):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    date_time_str = datetime.datetime.now().strftime(\"%m.%d, %H-%M\")\n",
    "    dim = args['n_embd']//args['n_head']\n",
    "    mode = args['mode']\n",
    "    filename = f\"{date_time_str}, {dim}-dim, {mode} ckpt.pt\"\n",
    "    checkpoint = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'model_args': model_args,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(out_dir, filename))\n",
    "#     print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "save_checkpoints = True\n",
    "out_dir = 'out'\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, epoch, overfit_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    val_loss = evaluate(model, epoch, overfit_val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "#     print(f\"Epoch: {epoch} | Training Loss: {train_loss} | Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Conditionally save checkpoints\n",
    "    if save_checkpoints:\n",
    "        save_checkpoint(model, optimizer, model_args, train_losses, val_losses, out_dir, args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# print(overfit_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc62ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
